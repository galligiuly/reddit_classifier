{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "proyect_testing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5FpHIeoCMsW",
        "colab_type": "text"
      },
      "source": [
        "# Project testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkyIJJvLBxcG",
        "colab_type": "code",
        "outputId": "b9f24cea-a169-474f-af9c-473040a8b372",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authenticated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkQ_JUZ7Cc7X",
        "colab_type": "code",
        "outputId": "115d6d61-da76-4c66-da69-392779c2b175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "!gcloud auth login"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?code_challenge=RapekuOl9OxKxJpNRddcmYlJ-Te8Ay9-zMUfGgQJpZo&prompt=select_account&code_challenge_method=S256&access_type=offline&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&client_id=32555940559.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth\n",
            "\n",
            "\n",
            "Enter verification code: 4/uwEOFqDUUMKjRc11zFYWnb9FaRO1-3GoF3ybzEaIWfIa8OvIp5HbcxA\n",
            "\u001b[1;33mWARNING:\u001b[0m `gcloud auth login` no longer writes application default credentials.\n",
            "If you need to use ADC, see:\n",
            "  gcloud auth application-default --help\n",
            "\n",
            "You are now logged in as [galli.giuly@gmail.com].\n",
            "Your current project is [None].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmHWk97rCrmm",
        "colab_type": "code",
        "outputId": "ca46c8ec-cb89-4376-9249-0cd479c6db7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%env GCLOUD_PROJECT=reddit-master"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: GCLOUD_PROJECT=reddit-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siW2dGCmCxI-",
        "colab_type": "code",
        "outputId": "35564c3e-4ff5-4034-912c-73627cb0f799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        }
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "import re\n",
        "import plotly.graph_objs as go\n",
        "import chart_studio.plotly as py\n",
        "import cufflinks\n",
        "import plotly.figure_factory as ff\n",
        "import logging\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import keras.backend as K\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import log_loss\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Input\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dropout\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "from bs4 import BeautifulSoup\n",
        "from plotly.offline import iplot\n",
        "cufflinks.go_offline()\n",
        "cufflinks.set_config_file(world_readable=True, theme='pearl')\n",
        "from tensorflow import metrics, local_variables_initializer\n",
        "from keras.models import load_model\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zHqOWRLxC2p",
        "colab_type": "text"
      },
      "source": [
        "### Downloading the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFDK9Un46xBR",
        "colab_type": "code",
        "outputId": "de617706-cc1f-4a63-e94e-64163460f4b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "def auc(y_true, y_pred):\n",
        "    auc = metrics.auc(y_true, y_pred)[1]\n",
        "    K.get_session().run(local_variables_initializer())\n",
        "    return auc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:111: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYILlS0FuCrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dependencies = {\n",
        "    'auc': auc\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLOe5cHBcJvM",
        "colab_type": "code",
        "outputId": "74bd30ac-c5e3-4676-d4ee-be4a32a40b27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!gsutil cp gs://reddit_models/auc_model_lstm_30_batchsize_150_10_subreddits.h5 ."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://reddit_models/auc_model_lstm_30_batchsize_150_10_subreddits.h5...\n",
            "| [1 files][ 86.0 MiB/ 86.0 MiB]                                                \n",
            "Operation completed over 1 objects/86.0 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2HXzHgT5kD9",
        "colab_type": "code",
        "outputId": "4f6e470a-60f5-4436-c0e9-bf79bcd51c41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "model = load_model(\"auc_model_lstm_30_batchsize_150_10_subreddits.h5\", custom_objects=dependencies)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/metrics_impl.py:808: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct7MYgkAEgW3",
        "colab_type": "code",
        "outputId": "52ed7e8d-be68-40f3-cdae-a3cdbeca48ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# importing my final tokenizer\n",
        "\n",
        "!gsutil cp gs://reddit_models/reddit_tokenizer.pkl ."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://reddit_models/reddit_tokenizer.pkl...\n",
            "\\ [1 files][ 47.0 MiB/ 47.0 MiB]                                                \n",
            "Operation completed over 1 objects/47.0 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlMZr9HeEgTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('reddit_tokenizer.pkl', 'rb') as file:\n",
        "    tokenizer = pkl.load(file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up4VThdhvDki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def label_decoder(subreddit):\n",
        "    dict_labels = {\n",
        "        0:\"Fitness\",\n",
        "        1:\"atheism\",\n",
        "        2:\"aww\",\n",
        "        3:\"europe\",\n",
        "        4:\"gaming\",\n",
        "        5:\"movies\",\n",
        "        6:\"nba\",\n",
        "        7:\"politics\",\n",
        "        8:\"science\",\n",
        "        9:\"technology\"\n",
        "    }\n",
        "    \n",
        "    return dict_labels[subreddit]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoIR9169Di_z",
        "colab_type": "text"
      },
      "source": [
        "# How test my model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS3cXnQ5Do_g",
        "colab_type": "text"
      },
      "source": [
        "Here below you can find the steps to follow for testing the model:\n",
        "\n",
        "    \n",
        "\n",
        "*   go to reddit.com and search for one of the following subreddits:\n",
        "\n",
        "        Fitness\n",
        "        atheism\n",
        "        aww\n",
        "        europe\n",
        "        gaming\n",
        "        movies\n",
        "        nba\n",
        "        politics\n",
        "        science\n",
        "        technology\n",
        "\n",
        "*   copy the text you want to test\n",
        "\n",
        "*   go to \"Step 1\" and paste the text once you have executed the correspoding row\n",
        "\n",
        "*   Execute step 2\n",
        "\n",
        "*   Execute step 3 for the result\n",
        "\n",
        "**Final test**: is the suggestion of my model exactly the subreddit where you picked the text?!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNTcTbjuEN0_",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: copy the text or post you what to publush"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7Ea5aNor-wF",
        "colab_type": "code",
        "outputId": "df07514c-f2b4-4032-d228-a6989250f2da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "text = input(\"Please, enter the text of your blog: \") \n",
        "print(\"\")\n",
        "print(\"Text correctly entered, I'll give you that subreddit in a minute\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please, enter the text of your blog: As you can see in the picture below, I made it to quite a few places over my time abroad. I started things off in Iceland for a short stay before I moved to Germany, followed by the Czech Republic and Austria. From there, I went down to Greece and Italy for nearly 3 weeks. I moved up back into Germany again and over to Hungary and Poland before I caught a flight to London. I went to Amsterdam, Brussels, and Paris for a bit and eventually made my way down to Spain where I ended up staying for over two weeks. Wrapping things up, I went over to Portugal and then spent some time in Ireland.\n",
            "\n",
            "Text correctly entered, I'll give you that subreddit in a minute\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySiRjUO0ERca",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: processing the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJEdzFrfvLO4",
        "colab_type": "code",
        "outputId": "992a47bd-8cee-4d48-e10e-ac3c6260a7e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "MAX_NB_WORDS = 75000\n",
        "MAX_SEQUENCE_LENGTH = 450\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "X = tokenizer.texts_to_sequences([text])\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor: (1, 450)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4yw3HfnETYe",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: get the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0S6eE5Dr-tR",
        "colab_type": "code",
        "outputId": "2cd69d95-0e70-45db-b64f-8ef02e9ae38e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "result = model.predict(X)\n",
        "\n",
        "def convert_to_cat(arr):\n",
        "    biggest = 0\n",
        "    for x in range(0, len(arr)):\n",
        "        if arr[x] > arr[biggest]:\n",
        "            biggest = x\n",
        "    return biggest\n",
        "\n",
        "category = [label_decoder(convert_to_cat(x)) for x in result]\n",
        "\n",
        "print(\"you should publish your text into the sureddit: \", category)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "you should publish your text into the sureddit:  ['europe']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}